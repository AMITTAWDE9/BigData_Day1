{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Apache Flume"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "Apache Flume is a distributed, reliable, and available system for efficiently collecting, aggregating and moving large amounts of log data from many different sources to a centralized data store. \n",
    "\n",
    "\n",
    "\n",
    "### Data stream management\n",
    "\n",
    "<img src = \"images/flume_ds.jpeg\">\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Flume Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src = \"images/Flume-Architecture.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Flume Components\n",
    "\n",
    "A Flume data flow is made up of five main components: **Events, Sources, Channels, Sinks, and Agents**:\n",
    "\n",
    "**Events** -  An event is the basic unit of data that is moved using Flume. It is similar to a message and is generally small. It is made up of headers and a byte-array body.\n",
    "\n",
    "**Sources** - The source receives the event from some external entity and stores it in a channel. The source must understand the type of event that is sent to it.\n",
    "\n",
    "**Channels** -  A channel is an internal passive store with certain specific characteristics. An in-memory channel, for example, can move events very quickly, but does not provide persistence. A file-based channel provides persistence. \n",
    "\n",
    "**Sinks** - The sink removes the event from the channel and forwards it to either to a destination, like HDFS, or to another agent/dataflow. The sink must output an event that is appropriate to the destination.\n",
    "\n",
    "**Agents** - An agent is the container for a Flume data flow. It is any physical JVM running Flume. An agent must contain at least one source, channel, and sink, but the same agent can run multiple sources, sinks, and channels. A particular data flow path is set up through the configuration process.\n",
    "\n",
    "Note: A source stores an event in the channel where it stays until it is consumed by a sink. This temporary storage lets source and sink run asynchronously.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Types of Sources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src = \"images/flume-type-sources.jpg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installation:\n",
    "\n",
    "**Step 1:** Download Flume from the below link:\n",
    "\n",
    "http://www-us.apache.org/dist/flume/1.8.0/apache-flume-1.8.0-bin.tar.gz\n",
    "\n",
    "**Step 2:** Untar the downloaded setup, after moving the setup to the desired location. Use the command given below to extract Apache flume :\n",
    "\n",
    "`tar xzf apache-flume-1.8.0-bin.tar.gz`\n",
    "\n",
    "**Step 3:** Make an entry for Apache flume into your .bashrc file. Open `.bashrc`:\n",
    "`nano ~/.bashrc`\n",
    "\n",
    "Then add the following lines:\n",
    "*Remember to give path where your apache flume is placed*\n",
    "\n",
    "`export FLUME_HOME=/home/prakshi/apache-flume-1.6.0-bin/`\n",
    "\n",
    "and then\n",
    "\n",
    "`export PATH=$PATH:$FLUME_HOME/bin/`\n",
    "\n",
    "\n",
    "The purpose of a .bashrc file is to provide a place where you can set up variables, functions and aliases, define other settings that you want to use every start you open a new terminal window.\n",
    "\n",
    "**Step 3:** Refresh .bashrc file execute this command- \n",
    "\n",
    "`source ~/.bashrc`\n",
    "\n",
    "**Step 4:** In order to verify that Flume has been successfully configured execute the below command on the terminal and if the below output gets appeared means, you had successfully installed and configured Flume.\n",
    "\n",
    "`flume-ng --help`\n",
    "\n",
    "\n",
    "Expected output:\n",
    "\n",
    "\n",
    "<img src=\"images/Install.png\">  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have apache flume installed we can start experimenting with the ways to ingest data. Here we'll be picking examples which can be performed without any special prerequisite so that everyone is able to do hands-on exercises.\n",
    "\n",
    "## Setup an Agent\n",
    "\n",
    "- Flume agent configuration is stored in a local configuration file. This is a text file that follows the Java properties file format. \n",
    "- Configurations for one or more agents can be specified in the same configuration file. \n",
    "- The configuration file includes properties of each source, sink and channel in an agent and how they are wired together to form data flows.\n",
    "- Each component (source, sink or channel) in the flow has a name, type, and set of properties that are specific to the type and instantiation. \n",
    "- All attributes of a component needs to be set in the properties file of the hosting Flume agent.\n",
    "- Each configuration file listing of the names of each of the sources, sinks and channels in the agent, and then specifying the connecting channel for each sink and source.\n",
    "\n",
    "**Let's write a configuration file to see how it works**\n",
    "\n",
    "Open up your favourite editor to write a `.conf`, let's name it `firstAgent.conf`. \n",
    "\n",
    "We'll take up Source for our agent as **netcat** and sink to be **logger** so that we can see data coming in real time.\n",
    "\n",
    "**netcat source**:\n",
    "\n",
    "\n",
    "A netcat-like source opens a specified port and listens for data and turns each line of text into an event. The expectation is that the supplied data is newline separated text. Each line of text is turned into a Flume event and sent via the connected channel.\n",
    "\n",
    "Below is the configuration file required to setup this netcat agent.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```bash\n",
    "# firstAgent.conf: A single-node Flume configuration\n",
    "\n",
    "# Name the components on this agent\n",
    "myAgent.sources = r1\n",
    "myAgent.sinks = k1\n",
    "myAgent.channels = c1\n",
    "\n",
    "# Describe/configure the source\n",
    "myAgent.sources.r1.type = netcat\n",
    "myAgent.sources.r1.bind = localhost\n",
    "myAgent.sources.r1.port = 55555\n",
    "\n",
    "# Describe the sink\n",
    "myAgent.sinks.k1.type = logger\n",
    "\n",
    "# Use a channel which buffers events in memory\n",
    "myAgent.channels.c1.type = memory\n",
    "myAgent.channels.c1.capacity = 1000\n",
    "myAgent.channels.c1.transactionCapacity = 100\n",
    "\n",
    "# Bind the source and sink to the channel\n",
    "myAgent.sources.r1.channels = c1\n",
    "myAgent.sinks.k1.channel = c1\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hang on! Let's have a section wise look to the instructions given in this conf file\n",
    "\n",
    "\n",
    "### First Section - Naming the components\n",
    "\n",
    "```bash\n",
    "# Name the components on this agent\n",
    "myAgent.sources = r1\n",
    "myAgent.sinks = k1\n",
    "myAgent.channels = c1\n",
    "```\n",
    "\n",
    "- The first qualifier in the above three lines is the agent name. We can assign any name of our choice to the agent. Here we have assigned a name to the agent as `myAgent`.\n",
    "\n",
    "- Second qualifier denotes any component among sources, channels and sinks. Here the keywords (sources, channels, sinks) used for second qualifier are fixed and these canâ€™t be replaced with any other names to refer the same components.\n",
    "\n",
    "- The value assigned on each line at the right side is the name given to each component for this agent. Here we have named, source as `r1`, sink as `k1` and the channel as `c1`. Names can be any strings without space in between. Strings seperated with space are considered to be multiple entries.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Second Section - Configure the source \n",
    "\n",
    "```bash\n",
    "#Describe/configure the source\n",
    "myAgent.sources.r1.type = netcat\n",
    "myAgent.sources.r1.bind = localhost\n",
    "myAgent.sources.r1.port = 55555\n",
    "\n",
    "```\n",
    "\n",
    "- Second section specify the configuration for the source. \n",
    "- First qualifier - Agent name.\n",
    "- Second qualifier - Reserved keyword for sources. \n",
    "- Third qualifier - Source name given in the first section.\n",
    "- Fourth qualifier specifies additional properties of source such as type, port and bind.\n",
    "- Right side values are specific values for source. We can specify as many properties as available for the source.\n",
    "- Here we are binding localhost:55555 with the netcat source.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Third section  - Describe the sink\n",
    "\n",
    "```bash\n",
    "myAgent.sinks.k1.type = logger\n",
    "```\n",
    "\n",
    "**Logger sink**:\n",
    "\n",
    "Logs event at INFO level. Typically useful for testing/debugging purpose. Logging the raw stream of data flowing through the ingest pipeline is not desired behaviour in many production environments because this may result in leaking sensitive data or security related configurations, such as secret keys, to Flume log files.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fourth Section - Channel specification\n",
    "\n",
    "```bash\n",
    "# Use a channel which buffers events in memory\n",
    "myAgent.channels.c1.type = memory\n",
    "myAgent.channels.c1.capacity = 1000\n",
    "myAgent.channels.c1.transactionCapacity = 100\n",
    "```\n",
    "\n",
    "**Memory Channel:**\n",
    "\n",
    "- Channels are the repositories where the events are staged on a agent\n",
    "- The events are stored in an in-memory queue with configurable max size.\n",
    "- Capacity refers to the maximum number of events stored in the channel.\n",
    "- While the transaction capacity refers to the maximum number of events the channel will take from a source or give to a sink per transaction.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Starting an agent\n",
    "An agent is started using a shell script called flume-ng which is located in the bin directory of the Flume distribution. You need to specify the agent name, the config directory, and the config file on the command line:\n",
    "\n",
    "\n",
    "` flume-ng agent --conf conf --conf-file firstAgent.conf --name myAgent -Dflume.root.logger=INFO,console`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Start producing some data at source**\n",
    "Since we have specified netcat as our source and binded 55555 port so let's post some data on this port:\n",
    "\n",
    "Open up another terminal and run the below command: \n",
    "\n",
    "`telnet localhost 55555`\n",
    "\n",
    "Next, you'll see a prompt to post some data at this port. Similar to this:\n",
    "\n",
    "<img src=\"images/netcat.png\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's switch to the terminal where we started our Flume agent and see how our posted data looks like. You'll see similar output\n",
    "\n",
    "<img src = \"images/myAgent.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Found something strange?**\n",
    "\n",
    "If you notice, only first 16 characters of your message is being displayed in the flume agent console. Does this mean that your rest message is lost?\n",
    "\n",
    "Answer to such behaviour is:\n",
    "\n",
    "The default logger sink truncates the body content to 16 bytes so only first 16 characters are visible on the console."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Flow chart describing a transaction event"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src = \"images/transact.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's do one more exercise with some other source.\n",
    "\n",
    "## Source type: Exec Source\n",
    "\n",
    "**Exec Source:** \n",
    "\n",
    "- Exec source runs a given Unix command on start-up and expects that process to continuously produce data on standard out (stderr is simply discarded, unless property logStdErr is set to true). \n",
    "\n",
    "- If the process exits for any reason, the source also exits and will produce no further data. This means configurations such as cat [named pipe] or tail -F [file] are going to produce the desired results where as date will probably not - the former two commands produce streams of data where as the latter produces a single event and exits.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Open up a new file, let's name it `example2.conf`. Here our source will be **exec** and we'll write the ingested data to a file instead of logging it into the console.\n",
    "\n",
    "```bash\n",
    "### Agent2 Configuration - Exec Source, File Roll Sink & File Channel ###\n",
    "\n",
    "# Name the components on this agent \n",
    "Agent2.sources = exec-source  \n",
    "Agent2.channels = file-channel\n",
    "Agent2.sinks = file-sink\n",
    "\n",
    "# Describe/configure Source\n",
    "Agent2.sources.exec-source.type = exec\n",
    "Agent2.sources.exec-source.command = tail -F /var/log/syslog\n",
    "\n",
    "# Describe the sink\n",
    "Agent2.sinks.file-sink.type = FILE_ROLL\n",
    "Agent2.sinks.file-sink.sink.directory = /home/prakshi/Prakshi/GreyAtom/flume_examples/op_files\n",
    "Agent2.sinks.file-sink.sink.rollInterval = 0\n",
    "\n",
    "# Use a channel which buffers events in file\n",
    "Agent2.channels.file-channel.type = file\n",
    "Agent2.channels.file-channel.checkpointDir = /home/prakshi/Prakshi/GreyAtom/flume_examples/checkpoint\n",
    "Agent2.channels.file-channel.dataDirs = /home/prakshi/Prakshi/GreyAtom/flume_examples/data/\n",
    "\n",
    "# Bind the source and sink to the channel\n",
    "Agent2.sources.exec-source.channels = file-channel\n",
    "Agent2.sinks.file-sink.channel = file-channel\n",
    "```\n",
    "\n",
    "**Note:** Make sure your `checkpontDir` and `dataDirs` are already created and accessible by the flume agent. Flume can only make files to write output but cannot create directory for you."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What does the `tail` command do?**\n",
    "\n",
    "The tail command is used to display contents of a file from the end. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Channel type: file-channel\n",
    "\n",
    "- Channels are the repositories where the events are staged on a agent\n",
    "- FileChannel is a persistent Flume channel that supports writing to multiple disks in parallel and encryption.\n",
    "- It writes out all events to disk and thus does not lose data on process or machine shutdown or crash.\n",
    "- The File Channel is designed to be used in situations where data durability is required and data loss cannot be tolerated.\n",
    "- The checkpoint reflects the exact state of the channel at the instant the checkpoint was written out.\n",
    "- `checkpointDir` - The directory where checkpoint file will be stored\n",
    "- On restart, the channel loads the last checkpoint written out and only replays the puts and takes that happened after this checkpoint, allowing the channel to start up quickly and be ready for normal operation. \n",
    "- `dataDirs` - Comma separated list of directories for storing log files. Using multiple directories on separate disks can improve file channel peformance\n",
    "\n",
    "\n",
    "Flume is a transactional system and multiple events can be either Put or Taken in a single transaction. The batch size can be used to control throughput. \n",
    "\n",
    "A Flume transaction consists of either Puts or Takes, but not both, and either a commit or a rollback. Each transaction implements both a Put and Take method. Sources do Puts onto the channel and Sinks do Takes from the channel.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start the Flume agent\n",
    "\n",
    "Start the flume agent for the second configuration file we prepared just now by giving the following command:\n",
    "\n",
    "`flume-ng agent --conf conf --conf-file example2.conf --name Agent2 -Dflume.root.logger=INFO,console`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Go to the location of `dataDirs` you specified in the configuration file. There you'll see a file created by the flume agent which contains the output of `tail` command which was originally written into the flume channel and passed to the sink. For example my file looks like as shown below and will contain system logs:\n",
    "\n",
    "<img src = \"images/tailOutput.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are many more types of Sources, channels and sink available which you can try in a similar manner. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to explore more sources then you can refer to the official documentation of Apache Flume which serve as a guide to setup an agent: [Apache Flume](https://flume.apache.org/FlumeUserGuide.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Memory Channel vs. File Channel\n",
    "\n",
    "An important decision to make when designing your Flume flow is what type of channel you want to use.\n",
    "\n",
    "- The file channel is a durable channel, as it persists all events that are stored in it to disk. So, even if the Java virtual machine is killed, or the operating system crashes or reboots, events that were not successfully transferred to the next agent in the pipeline will still be there when the Flume agent is restarted. \n",
    "\n",
    "- The memory channel is a volatile channel, as it buffers events in memory only: if the Java process dies, any events stored in the memory channel are lost. Naturally, the memory channel also exhibits very low put/take latencies compared to the file channel, even for a batch size of 1. \n",
    "\n",
    "- Since the number of events that can be stored is limited by available RAM, its ability to buffer events in the case of temporary downstream failure is quite limited. The file channel, on the other hand, has far superior buffering capability due to utilizing cheap, abundant hard disk space."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
