{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Big Data Architectures "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take an example, a company wants to start building up **Big Data architectures**.\n",
    "\n",
    "**What can be a reason behind this shift?**\n",
    "\n",
    "Companies nowadays collect a tremendous amount of data on their customers. From purchase history to social media commentary, customer insights may be collected across multiple touchpoints. In addition, contact center metrics such as average handling time and first contact resolution provide data on how the customer experience is affected by service practices. The task here is to take their Website logs and predict customer behaviour of consuming their services. Say for example, answering question like, how much time on an average a customer spents there, what products are hit most and where do their search ends.\n",
    "\n",
    "**What tasks you think are invovled in building an architecture for handling this Big Data problem?**\n",
    "\n",
    "The best way to propose a solution to a Big Data problem is to divide it into layers of operations. This course will describe on desigining Big Data Pipeline which itself consists of layers as follows:\n",
    "\n",
    "1. Data Ingestion Layer\n",
    "2. Data Collector Layer\n",
    "3. Data Processing Layer\n",
    "4. Data Storage Layer\n",
    "5. Data Query Layer\n",
    "6. Data Visualization Layer\n",
    "\n",
    "Let's have a quick look to all the layers so that we can start up on building one of ours. Detailed explanation for each layer will be provided in their respective section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Big Data layerd architecture](images/layers.png \"Big Data layerd architecture\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. Data Ingestion Layer**\n",
    "\n",
    "Big Data Ingestion involves connecting to various data sources, extracting the data, and detecting the changed data. It's about moving data - and especially the unstructured data - from where it is originated, into a system where it can be stored and analyzed.\n",
    "\n",
    "**2. Data Collector Layer**\n",
    "\n",
    "Data Collector layer comes into play when there are multiple sources ingesting Data into your pipeline. Before starting any analysis it's necessary to collect all data sources into one and bring to a format which further pipeline can take easily without making the architecture complex. This can only happen when all the data ingestion is finally being collected at one place. Essentialy the focus is on the transportation of data from ingestion layer to rest of data pipeline. \n",
    "\n",
    "**3. Data Processing Layer**\n",
    "\n",
    "This is layer where a Data Scientist can play with data as much desired. Keeping this in mind, the course will take up this layer from basic to advance level. Now, we have all the data in required format and next we need to design parallel processing on the data and  get some output to serve our purpose of the Data Analysis done so far. Workflow is very simple - Input --> Process --> Output.\n",
    "\n",
    "**4. Data Storage Layer**\n",
    "\n",
    "Storage becomes a challenge when the size of the data you are dealing with, becomes large. Several possible solutions can rescue from such problems. Finding a storage solution is very much important when the size of your data becomes large. This layer focuses on **\"where to store such a large data in an efficient, scalable, easily accesible and secured manner\"**.\n",
    "\n",
    "**5. Data Query Layer**\n",
    "\n",
    "This is the layer where strong analytic processing takes place. Data analytics is an\n",
    "essential step which solved the inefficiencies of traditional data platforms to handle\n",
    "large amounts of data related to interactive queries, ETL, storage and processing \n",
    "\n",
    "**6. Data Visualization Layer**\n",
    "\n",
    "This layer focus on Big Data Visualization. We need something that will grab\n",
    "people’s attention, pull them in, make your findings well-understood. This is the\n",
    "where the data value is perceived by the user.\n",
    "\n",
    "• **Dashboards** – Save, share, and communicate insights. It helps users generate\n",
    "questions by revealing the depth, range, and content of their data stores.\n",
    "\n",
    "• **Recommenders** - Recommender systems focus on the task of information\n",
    "filtering, which deals with the delivery of items selected from a large collection\n",
    "that the user is likely to find interesting or useful.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "# Programming Models for Big Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why do we need a Programming model?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given the large volume of data, applications that work on big data need to distribute data on a cluster of processors, and processing has to be carried out in parallel for computation to complete in a reasonable amount of time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is the challenge for developers?\n",
    "\n",
    "\n",
    "Distributed applications require a developer to orchestrate concurrent computation and communication across machines, in a manner that is robust to delays and failures. This adds a overhead of maintaining the infrastructure demands and give considerate time to deploying the application and not just developing it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is the solution?\n",
    "\n",
    "- A programmer is provided high level primitives (API) to express computation tasks;\n",
    "- The model automatically parallelize the tasks and executes them on a cluster of shared nothing commodity compute nodes.\n",
    "- Focus on the solution of the problem rather than the mundane tasks of parallelization\n",
    "\n",
    "### Define: Programming Model for Big Data\n",
    "\n",
    "- A programming model is an abstraction or existing machinery or infrastructure. It is a set of abstract runtime libraries and programming languages that form a model of computation.\n",
    "\n",
    "- If the enabling infrastructure for big data analysis is distributed file systems, then the programming model for big data should enable the programmability of the operations within distributed file systems.\n",
    "\n",
    "- Enable a developer to write computer programs that work efficiently on top of distributed file systems using big data.\n",
    "\n",
    "-  In big data programming, users focus on writing data-driven parallel programs which can be executed on large scale and distributed environments\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we'll see three major programming models for writing big data applications:\n",
    "\n",
    "**1. Distributed File System**\n",
    "\n",
    "**2. MapReduce**\n",
    "\n",
    "**3. Functional Programming**\n",
    "\n",
    "Let's dig into details\n",
    "\n",
    "### 1. Distributed File System\n",
    "\n",
    "- In a distributed file system, Data sets, or parts of a data set, can be replicated across the multiple nodes of a cluster.\n",
    "\n",
    "- Distributed file systems replicate the data between the racks, and also computers distributed across geographical regions.\n",
    "\n",
    "- Since data is already on these nodes, then analysis of parts of the data is needed in a data parallel fashion, computation can be moved to these nodes.  \n",
    "- Data replication makes the system more fault tolerant.\n",
    "\n",
    "- That means, if some nodes or a rack goes down, there are other parts of the system, the same data can be found and analyzed.\n",
    "\n",
    "- Data replication also helps with scaling the access to this data by many users.\n",
    "\n",
    "- Hadoop DFS has a master/slave architecture. An HDFS cluster consists of a **single NameNode**, a master server that manages the file system namespace and regulates access to files by clients. In addition, there are **a number of DataNodes** , usually one per node in the cluster, which manage storage attached to the nodes that they run on. HDFS exposes a file system namespace and allows user data to be stored in files. The DataNodes also perform block creation, deletion, and replication upon instruction from the NameNode.\n",
    "\n",
    "#### A simple use case\n",
    "\n",
    "- Suppose you have enormous amount of data generated on a regular basis and so you need to store it somewhere. There are two ways to approach this problem. \n",
    "    \n",
    "    1. First one, a straight approach to store it on a big storage capacity node, also called **vertical scaling**.\n",
    "    2. The second one is to store it on a collection of nodes, also called **horizontal scaling**.\n",
    "    \n",
    "    **What if we want to store more data?** \n",
    "    \n",
    "    \n",
    "        1.  Buy more storage. But remember, nothing is infinite. For instance, Facebook had a storage of 300 petabytes of data in 2014. Can you think of scaling up to this limit with a hard drive?\n",
    "    \n",
    "        2.  Here Horizontal scaling and Distributed file system can together help us. Need to store more data? Let's take a cluster of 1000 nodes but here is a good probability that each node will get out service once in three years on an average.Thus with a cluster of 1,000 nodes you will get one pillar each day, approximately which may lead to data loss. That is where the replication property of distributed file system save us from data loss. Each scaling approach has its own pros and cons. Accessing data you usually get lower latency with vertical scaling. You get higher latency with horizontal scale but you can build a bigger storage solution. \n",
    "        \n",
    "        \n",
    "<img src=\"images/scaling.png\">\n",
    "\n",
    "***Fig: Vertical VS Horizontal Scaling***\n",
    "\n",
    "\n",
    "\n",
    "***Or we can think of this in a non-technical fashion as shown below:***\n",
    "\n",
    "\n",
    "<img src=\"images/scaling.jpg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. MapReduce\n",
    "- Enables writing data-centric parallel applications.\n",
    "- MapReduce is inspired by the commonly used functions - Map and Reduce in combination with the **divide-and-conquer parallel paradigm**. \n",
    "- For a single MapReduce job, users implement two basic procedure objects **Mapper and Reducer** for different processing stages as shown in Figure below.\n",
    "- Then the MapReduce program is automatically interpreted by the execution engine and executed in parallel in a distributed environments.\n",
    "- **Key-Value based**: In MapReduce, both input and output data are considered as Key-Value pairs with different types.\n",
    "\n",
    "#### A simple use case\n",
    "\n",
    "- Suppose you have a dataset of videos getting upload to Youtube and you aim to find out what are the top 5 categories with maximum number of videos uploaded. How will you proceed?\n",
    " \n",
    " \n",
    "- Now from the mapper, we can get the video category from each video and map it into a key-value pair form with video category as key and related value ‘1’ as values which will be passed to the reducer step next.\n",
    "\n",
    "\n",
    "- Reducer can finally aggregate the data at one place and map the count to each video category."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Another Map Reduce example**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/MapReduce.jpg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Functional Programming\n",
    "\n",
    "- In functional programming, programming interfaces are specified as functions that applied on input data sources.\n",
    "\n",
    "- The computation is treated as a calculation of functions.\n",
    "\n",
    "- Functional programming itself is declarative and it avoids mutable states sharing. Compared to Object-oriented Programming it is more compact and intuitive for representing data driven transformations and applications\n",
    "\n",
    "- Wide application will be seen while working with Apache Spark. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src = \"images/funcProg.jpg\">"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
